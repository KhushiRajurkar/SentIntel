{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34d1f34-b37b-4938-9581-815846462666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W424 14:24:00.736874296 OperatorEntry.cpp:154] Warning: Warning only once for all operators,  other operators may also be overridden.\n",
      "  Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::_validate_compressed_sparse_indices(bool is_crow, Tensor compressed_idx, Tensor plain_idx, int cdim, int dim, int nnz) -> ()\n",
      "    registered at /pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: XPU\n",
      "  previous kernel: registered at /pytorch/build/aten/src/ATen/RegisterCPU.cpp:30477\n",
      "       new kernel: registered at /build/intel-pytorch-extension/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:468 (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 14:24:03,226] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "2025-04-24 14:24:11,617 - datasets - INFO - PyTorch version 2.6.0+xpu available.\n"
     ]
    }
   ],
   "source": [
    "# ─── 0) Imports ────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "!{sys.executable} -m pip install transformers datasets evaluate --quiet\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, RobertaModel,\n",
    "                          TrainingArguments, Trainer)\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e6e139-3c2d-4283-a05e-d6ef101f823e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n"
     ]
    }
   ],
   "source": [
    "# ✅ Use Intel GPU if available\n",
    "device = torch.device(\"xpu\" if hasattr(torch, \"xpu\") and torch.xpu.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cd1af7-ee28-4c9b-9963-5de3d98eedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1) FocalLoss Definition ─────────────────────────────────────────────\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        BCE = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        p_t = torch.exp(-BCE)\n",
    "        loss = self.alpha * (1 - p_t) ** self.gamma * BCE\n",
    "        return loss.mean() if self.reduction==\"mean\" else loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6314a14-02aa-4a7f-8abf-ea21ddbc638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoEmotions has 28 labels: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "# ─── 2) Load & preprocess dataset ────────────────────────────────────────\n",
    "dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "\n",
    "# 📝 Grab and print the human-readable label names before we overwrite them\n",
    "emotion_labels = dataset[\"train\"].features[\"labels\"].feature.names\n",
    "print(f\"GoEmotions has {len(emotion_labels)} labels: {emotion_labels}\")\n",
    "\n",
    "# Now convert each example’s label-list into a 28-dim multi-hot vector\n",
    "import numpy as np\n",
    "\n",
    "def make_multihot(example):\n",
    "    mh = np.zeros(len(emotion_labels), dtype=np.int64)\n",
    "    for lbl in example[\"labels\"]:\n",
    "        mh[lbl] = 1\n",
    "    example[\"labels\"] = mh\n",
    "    return example\n",
    "\n",
    "# map example-by-example so we can read 'labels' as a list of ints\n",
    "dataset = dataset.map(make_multihot, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e634a09-b154-467e-88cf-ac6094ff2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 3) Compute per-label pos_weight & move to device ────────────────────\n",
    "all_labels = np.stack(dataset[\"train\"][\"labels\"])\n",
    "pos_weight = (all_labels.shape[0] - all_labels.sum(axis=0)) / (all_labels.sum(axis=0) + 1e-12)\n",
    "pos_weight = torch.tensor(pos_weight, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07cb954-96de-4021-bc80-5abee3b896db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463919f379754c26a12582c6798b77ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 4) Tokenize ─────────────────────────────────────────────────────────\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    return tokenizer(ex[\"text\"],\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=128)\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "tokenized.set_format(type=\"torch\",\n",
    "                     columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b346041e-4a85-49d3-9bd2-1a26192b214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 5) Threshold‐finding & compute_metrics ───────────────────────────────\n",
    "def find_best_threshold(logits, labels):\n",
    "    best, best_thr = 0, 0.5\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    for thr in np.linspace(0.1, 0.9, 17):\n",
    "        preds = (probs > thr).cpu().numpy().astype(int)\n",
    "        f1 = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "        if f1 > best:\n",
    "            best, best_thr = f1, thr\n",
    "    return best_thr\n",
    "\n",
    "optimal_threshold = None\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    global optimal_threshold\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    if optimal_threshold is None:\n",
    "        optimal_threshold = find_best_threshold(logits, labels)\n",
    "\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    preds = (probs > optimal_threshold).astype(int)\n",
    "\n",
    "    micro = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "    hamming = accuracy_score(labels.flatten(), preds.flatten())\n",
    "    return {\"micro_f1\": micro, \"hamming_acc\": hamming, \"threshold\": optimal_threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fb6ed2-9f62-4a7a-9f94-4e49d65f793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 6) Model Definition ─────────────────────────────────────────────────\n",
    "class RobertaForMultiLabel(nn.Module):\n",
    "    def __init__(self, num_labels=28):\n",
    "        super().__init__()\n",
    "        self.roberta    = RobertaModel.from_pretrained(checkpoint).to(device)\n",
    "        self.dropout    = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "        # switch to FocalLoss or BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        self.loss_fct   = FocalLoss(alpha=1.0, gamma=2.0)\n",
    "        # self.loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "        logits  = self.classifier(self.dropout(outputs))\n",
    "        loss    = self.loss_fct(logits, labels.float().to(device)) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f04602a0-46c7-4779-93ef-e7a87721cb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/opt/intel/miniforge3/envs/pytorch-gpu/lib/python3.11/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ─── 7) Training Arguments & Trainer ─────────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"goemotions_multilabel_model\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=RobertaForMultiLabel(num_labels=28),\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d880b212-bb26-4b12-a0b6-6521c6af7fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 14:58:06,297 - _logger.py - IPEX - INFO - Currently split master weight for xpu only support sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10854' max='10854' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10854/10854 13:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Hamming Acc</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.022553</td>\n",
       "      <td>0.600081</td>\n",
       "      <td>0.967510</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>0.021763</td>\n",
       "      <td>0.614800</td>\n",
       "      <td>0.968622</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10854, training_loss=0.024539222413276208, metrics={'train_runtime': 804.9374, 'train_samples_per_second': 107.859, 'train_steps_per_second': 13.484, 'total_flos': 0.0, 'train_loss': 0.024539222413276208, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─── 8) Train ────────────────────────────────────────────────────────────\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74ceb82d-4109-4c43-bb7f-5d2fa38cd3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ckpt: goemotions_multilabel_model/checkpoint-10854\n"
     ]
    }
   ],
   "source": [
    "best_ckpt = trainer.state.best_model_checkpoint\n",
    "print(\"Best ckpt:\", best_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52889972-c39d-43b7-96d9-7e799bec6f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
